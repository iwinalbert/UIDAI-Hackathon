{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqbBjvDlT2Hg",
        "outputId": "6ae0b087-9c9c-4b51-dbe6-401f2c8939e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-8.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.1)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (6.0.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (4.13.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (11.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.3)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (3.9.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-3.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.20.2)\n",
            "Downloading duckduckgo_search-8.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.3-py3-none-any.whl (14 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-3.0.1-py2.py3-none-any.whl (4.5 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=f330d3b1343c3f3c0c5432e75a28cd107ed466e1fec4bde9d9af04dc1ea1fde1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/91/9f/00d66475960891a64867914273fcaf78df6cb04d905b104a2a\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=d8f76d8df9496f076affd991f6e21500471a4a37774b69d44d3be7256930bc66\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/9f/fb/364871d7426d3cdd4d293dcf7e53d97f160c508b2ccf00cc79\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=a52693d9581f1608634d94a8ea0cbddba73c33ac1c0c5df85f3f6abc2278837c\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/72/f7/fff392a8d4ea988dea4ccf9788599d09462a7f5e51e04f8a92\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=adc341e286f57214eb688aaf225a6ae72059d0f2906a6dc18881e901087fb2a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, primp, lxml_html_clean, feedparser, cssselect, requests-file, feedfinder2, duckduckgo-search, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.3.0 duckduckgo-search-8.1.1 feedfinder2-0.0.4 feedparser-6.0.12 jieba3k-0.35.1 lxml_html_clean-0.4.3 newspaper3k-0.2.8 primp-0.15.0 requests-file-3.0.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.1\n"
          ]
        }
      ],
      "source": [
        "pip install duckduckgo-search newspaper3k lxml_html_clean requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from newspaper import Article\n",
        "from datetime import datetime\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION (EDIT THIS)\n",
        "# ==========================================\n",
        "\n",
        "# ğŸ”‘ PASTE YOUR SERPER API KEY HERE\n",
        "SERPER_API_KEY = \"e34895fc6c85eb3738dcde0e0440be9c80687c15\"\n",
        "\n",
        "# ğŸ¯ YOUR KEYWORDS\n",
        "KEYWORDS = [\n",
        "    \"aadhaar enrollment decrease reasons\",\n",
        "    \"aadhaar update pending issues\",\n",
        "    \"aadhaar voter id link problems\",\n",
        "    \"aadhaar authentication failure causes\",\n",
        "    \"uidai server downtime history\",\n",
        "    \"aadhaar biometric mismatch cases\",\n",
        "    \"aadhaar address update rejected reasons\"\n",
        "]\n",
        "\n",
        "# ğŸŒ OPTIONAL: LIMIT TO SPECIFIC SITES (Leave empty [] to search the whole web)\n",
        "# Adding sites makes the search stricter but more relevant.\n",
        "TARGET_SITES = [\n",
        "    \"thehindu.com\",\n",
        "    \"indianexpress.com\",\n",
        "    \"timesofindia.indiatimes.com\",\n",
        "    \"livemint.com\",\n",
        "    \"economictimes.indiatimes.com\"\n",
        "]\n",
        "\n",
        "# âš™ï¸ SETTINGS\n",
        "RESULTS_PER_KEYWORD = 20  # Max is 100 for Serper, but 10-20 is faster\n",
        "OUTPUT_FILE = \"aadhaar_large_dataset.json\"\n",
        "\n",
        "# ==========================================\n",
        "# 2. HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def get_google_search_results(query, num_results):\n",
        "    \"\"\"\n",
        "    Fetches search results from Serper.dev API.\n",
        "    \"\"\"\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "\n",
        "    # Construct the query. If sites are defined, append them (OR logic)\n",
        "    final_query = query\n",
        "    if TARGET_SITES:\n",
        "        site_operator = \" OR \".join([f\"site:{site}\" for site in TARGET_SITES])\n",
        "        final_query = f\"{query} ({site_operator})\"\n",
        "\n",
        "    payload = json.dumps({\n",
        "        \"q\": final_query,\n",
        "        \"num\": num_results\n",
        "    })\n",
        "\n",
        "    headers = {\n",
        "        'X-API-KEY': SERPER_API_KEY,\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, data=payload)\n",
        "        response.raise_for_status() # Raise error for bad responses (401, 403, 500)\n",
        "        return response.json().get(\"organic\", [])\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ API Error: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_article_details(url):\n",
        "    \"\"\"\n",
        "    Uses Newspaper3k to download and parse the article text and date.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        # Try to find the date\n",
        "        pub_date = article.publish_date\n",
        "        date_str = pub_date.strftime(\"%Y-%m-%d\") if pub_date else \"Unknown Date\"\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"title\": article.title,\n",
        "            \"date\": date_str,\n",
        "            \"text_snippet\": article.text[:500] # First 500 chars\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"failed\", \"error\": str(e)}\n",
        "\n",
        "# ==========================================\n",
        "# 3. MAIN EXECUTION LOOP\n",
        "# ==========================================\n",
        "\n",
        "def main():\n",
        "    all_data = []\n",
        "    total_keywords = len(KEYWORDS)\n",
        "\n",
        "    print(f\"ğŸš€ Starting Large Scale Scrape...\")\n",
        "    print(f\"ğŸ”‘ API Key: {'*' * 10}{SERPER_API_KEY[-4:]}\")\n",
        "    print(f\"ğŸ“š Keywords: {total_keywords}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for index, keyword in enumerate(KEYWORDS):\n",
        "        print(f\"\\n[{index + 1}/{total_keywords}] Searching: '{keyword}'...\")\n",
        "\n",
        "        # 1. Get Links from API\n",
        "        search_results = get_google_search_results(keyword, RESULTS_PER_KEYWORD)\n",
        "\n",
        "        if not search_results:\n",
        "            print(\"   âŒ No results found or API error.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"   found {len(search_results)} links. Extracting content...\")\n",
        "\n",
        "        # 2. Process Each Link\n",
        "        for result in search_results:\n",
        "            link = result.get('link')\n",
        "\n",
        "            # Skip if we already have this link (simple de-duplication)\n",
        "            if any(d['url'] == link for d in all_data):\n",
        "                continue\n",
        "\n",
        "            # Extract content\n",
        "            details = extract_article_details(link)\n",
        "\n",
        "            if details['status'] == 'success':\n",
        "                entry = {\n",
        "                    \"keyword\": keyword,\n",
        "                    \"title\": details['title'],\n",
        "                    \"url\": link,\n",
        "                    \"event_date\": details['date'], # Publish date as proxy\n",
        "                    \"snippet\": result.get('snippet', ''), # Google's snippet\n",
        "                    \"full_content\": details['text_snippet']\n",
        "                }\n",
        "                all_data.append(entry)\n",
        "                # print(f\"     âœ… {details['title'][:40]}... [{details['date']}]\")\n",
        "            else:\n",
        "                # print(f\"     âš ï¸ Failed to parse: {link}\")\n",
        "                pass\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. SAVE OUTPUT\n",
        "    # ==========================================\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(f\"ğŸ‰ Scraping Complete!\")\n",
        "    print(f\"ğŸ“„ Total Articles Saved: {len(all_data)}\")\n",
        "\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_data, f, indent=4, default=str)\n",
        "\n",
        "    print(f\"ğŸ’¾ Data saved to: {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXpZgY8AXL6t",
        "outputId": "48095e4e-5a7b-47a7-d5ba-9762240df933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting Large Scale Scrape...\n",
            "ğŸ”‘ API Key: **********7c15\n",
            "ğŸ“š Keywords: 7\n",
            "----------------------------------------\n",
            "\n",
            "[1/7] Searching: 'aadhaar enrollment decrease reasons'...\n",
            "   found 10 links. Extracting content...\n",
            "\n",
            "[2/7] Searching: 'aadhaar update pending issues'...\n",
            "   found 10 links. Extracting content...\n",
            "\n",
            "[3/7] Searching: 'aadhaar voter id link problems'...\n",
            "   found 10 links. Extracting content...\n",
            "\n",
            "[4/7] Searching: 'aadhaar authentication failure causes'...\n",
            "   found 10 links. Extracting content...\n",
            "\n",
            "[5/7] Searching: 'uidai server downtime history'...\n",
            "   found 10 links. Extracting content...\n",
            "\n",
            "[6/7] Searching: 'aadhaar biometric mismatch cases'...\n",
            "   found 10 links. Extracting content...\n",
            "\n",
            "[7/7] Searching: 'aadhaar address update rejected reasons'...\n",
            "   found 10 links. Extracting content...\n",
            "\n",
            "========================================\n",
            "ğŸ‰ Scraping Complete!\n",
            "ğŸ“„ Total Articles Saved: 68\n",
            "ğŸ’¾ Data saved to: aadhaar_large_dataset.json\n"
          ]
        }
      ]
    }
  ]
}